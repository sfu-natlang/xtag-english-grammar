\chapter{Evaluation and Results}
\label{evaluation}

In this appendix we describe various evaluations done of the XTAG
grammar. Some of these evaluations were done on an earlier version of
the XTAG grammar (the 1995 release), while other were done more
recently. We will try to indicate in each section which version was
used. 

\section{Parsing Corpora}

In the XTAG project, we have used corpus analysis in two main ways:
(1) to measure the performance of the English grammar on a given genre
and (2) to identify gaps in the grammar. 

The second type of evaluation involves performing detailed error
analysis on the sentences rejected by the parser, and we have done
this several times on WSJ and Brown data.
%Lifted directly from TAG+ paper
Based on the results of such analysis, we prioritize upcoming grammar
development efforts. The results of a recent error analysis are shown
in Table \ref{errors}.  The table does not show errors in parsing due
to mistakes made by the POS tagger which contributed the largest
number of errors: 32. At this point, we have added a treatment of
punctuation to handle \#1, an analysis of time NPs (\#2), a large
number of multi-word prepositions (part of \#3), gapless relative
clauses (\#7), bare infinitives (\#14) and have added the missing
subcategorization (\#3) and missing lexical entry (\#12).  We are in
the process of extending the parser to handle VP coordination (\#9)
(See Section~\ref{conjunction} on recent work to handle VP and other
predicative coordination). We find that this method of error analysis
is very useful in focusing grammar development in a productive
direction.

\begin{table}[htb]
\centering
\begin{tabular}{|l|l|l|} \hline
Rank & No of errors & Category of error \\ \hline
\#1  & 11  &    Parentheticals and appositives \\ \hline
\#2  & 8     &  Time NP \\ \hline
\#3  & 8  &     Missing subcat \\ \hline
\#4  & 7 &      Multi-word construction \\ \hline
\#5  & 6 &       Ellipsis \\ \hline
\#6  & 6  &      Not sentences \\ \hline
\#7  & 3  &      Relative clause with no gap \\ \hline
\#8  & 2  &      Funny coordination \\ \hline
\#9  & 2  &      VP coordination \\ \hline
\#10  & 2  &      Inverted predication \\ \hline
\#11  & 2  &      Who knows \\ \hline
\#12  & 1  &      Missing entry \\ \hline
\#13  & 1   &     Comparative? \\ \hline
\#14  & 1    &    Bare infinitive \\ \hline
\end{tabular}
\caption{Results of Corpus Based Error Analysis}
\label{errors}
\end{table}

To ensure that we are not losing coverage of certain phenomena as we
extend the grammar, we have a benchmark set of grammatical and
ungrammatical sentences from this technical report. We parse these
sentences periodically to ensure that in adding new features and
constructions to the grammar, we are not blocking previous analyses.
There are approximately $590$ example sentences in this set.

\section{TSNLP}

In addition to corpus-based evaluation, we have also run the English
Grammar on the Test Suites for Natural Language Processing (TSNLP)
English corpus \cite{Lehmann96}. The corpus is intended to be a
systematic collection of English grammatical phenomena, including
complementation, agreement, modification, diathesis, modality, tense
and aspect, sentence and clause types, coordination, and negation. It
contains 1409 grammatical sentences and phrases and 3036 ungrammatical
ones.

\begin{table*}[htb]
\centering
\begin{tabular}{|l|c|c|}
\hline
Error Class & \% & Example \\ \hline
POS Tag &  19.7\% & She adds  to/V it , He noises/N him abroad \\ \hline
Missing lex item & 43.3\% & {\it used} as an auxiliary V, {\it calm NP down} \\ \hline
Missing tree & 21.2\% & {\it should've}, {\it bet NP NP S}, {\it
regard NP as Adj} \\ \hline
Feature clashes & 3\% & {\it My every firm}, {\it All money} \\ \hline
Rest&12.8\% & {\it approx}, {\it e.g.} \\
\hline
\end{tabular}
\caption{Breakdown of TSNLP Errors}
\label{tsnlp-table}
\end{table*} 

%Before parsing the grammatical subset of the TSNLP data, we made a few
%tokenization changes: we changed contractions from two tokens to one,
%downcased the first words of sentences, changed a pair of square
%brackets to parentheses and changed quotes to pairs of opens and
%closes. 
There were 42 examples which we judged ungrammatical, and removed from
the test corpus. These were sentences with conjoined subject pronouns,
where one or both were accusative, e.g. {\it Her and him succeed.}
Overall, we parsed 61.4\% of the 1367 remaining sentences and
phrases. The errors were of various types, broken down in
Table~\ref{tsnlp-table}. As with the error analysis described above,
we used this information to help direct our grammar development
efforts. It also highlighted the fact
that our grammar is heavily slanted toward American English---our
grammar did not handle {\it dare} or {\it need} as auxiliary verbs,
and there were a number of very British particle constructions,
e.g. {\it She misses him out}. 

One general problem with the test-suite is that it uses a very
restricted lexicon, and if there is one problematic lexical item it is
likely to appear a large number of times and cause a disproportionate
amount of grief. {\it Used to} appears 33 times and we got all 33
wrong. However, it must be noted that the XTAG grammar has analyses
for syntactic phenomena that were not represented in the TSNLP test
suite such as sentential subjects and subordinating clauses among
others. This effort was, therefore, useful in highlighting some
deficiencies in our grammar, but did not provide the same sort of
general evaluation as parsing corpus data.

\section{Comparative Evaluation via Text Chunking}

We evaluated the XTAG parser for the text chunking
task~\cite{abney91}. In particular, we compared NP chunks and verb
group (VG) chunks\footnote{We treat a sequence of verbs and verbal
  modifiers, including auxiliaries, adverbs, modals as constituting a
  verb group.}  produced by the XTAG parser with the NP and VG chunks
from the Penn Treebank~\cite{marcus93}. The test involved $940$
sentences of length $15$ words or less from sections $17$ to $23$ of
the Penn Treebank, parsed using the XTAG English grammar. The results
are given in Table~\ref{chunking-results}.

\begin{table*}[htb]
\centering
\begin{tabular}{|l|c|c|}
\hline
& NP Chunking & VG Chunking \\ \hline
Recall & 82.15\% & 74.51\% \\ \hline
Precision & 83.94\%  & 76.43\% \\ \hline
\end{tabular}
\caption{Text Chunking performance of the XTAG parser}
\label{chunking-results}
\end{table*} 

\begin{table*}[htb]
\centering
\begin{tabular}{|c|c|c|c|} \hline
System & Training Size & Recall & Precision  \\ \hline \hline
Ramshaw \& Marcus & Baseline & 81.9\% & 78.2\% \\ \hline
Ramshaw \& Marcus & 200,000 & 90.7\% & 90.5\% \\ 
(without lexical information) & & & \\ \hline 
Ramshaw \& Marcus & 200,000 & 92.3\% & 91.8\% \\ 
(with lexical information) & & & \\ \hline \hline
Supertags & Baseline & 74.0\% & 58.4\% \\ \hline
Supertags & 200,000 & 93.0\% & 91.8\% \\ \hline
Supertags & 1,000,000 & 93.8\% & 92.5\% \\ \hline
\end{tabular}
\caption{Performance comparison of the transformation based noun
chunker and the supertag based noun chunker}
\label{nc-compare}
\end{table*}

As described earlier, the results cannot be directly compared with
other results in chunking such as in~\cite{lance&mitch95} since we do
not train from the Treebank before testing. However, in earlier work,
text chunking was done using a technique called
supertagging~\cite{srini97iwpt} (which uses the XTAG English grammar)
which can be used to train from the Treebank.  The comparative results
of text chunking between supertagging and other methods of chunking is
shown in Figure~\ref{nc-compare}.\footnote{It is important to note in
  this comparison that the supertagger uses lexical information on a
  per word basis only to pick an initial set of supertags for a given
  word.}

We also performed experiments to determine the accuracy of the
derivation structures produced by XTAG on WSJ text, where the
derivation tree produced after parsing XTAG is interpreted as a
dependency parse. We took sentences that were $15$ words or less from
the Penn Treebank~\cite{marcus93}. The sentences were collected from
sections $17$--$23$ of the Treebank. $9891$ of these sentences were
given at least one parse by the XTAG system. Since XTAG typically
produces several derivations for each sentence we simply picked a
single derivation from the list for this evaluation. Better results
might be achieved by ranking the output of the parser using the sort
of approach described in~\cite{srinietal95}.

There were some striking differences in the dependencies implicit in
the Treebank and those given by XTAG derivations. For instance, often
a subject NP in the Treebank is linked with the first auxiliary verb
in the tree, either a modal or a copular verb, whereas in the XTAG
derivation, the same NP will be linked to the main verb. Also XTAG
produces some dependencies within an NP, while a large number of words
in NPs in the Treebank are directly dependent on the verb. To
normalize for these facts, we took the output of the NP and VG chunker
described above and accepted as correct any dependencies that were
completely contained within a single chunk.

For example, for the sentence {\em Borrowed shares on the Amex rose to
another record}, the XTAG and Treebank chunks are shown below.

\begin{verbatim}
XTAG chunks:     
 [Borrowed shares] [on the Amex] [rose] 
    [to another record] 
Treebank chunks: 
 [Borrowed shares on the Amex] [rose] 
    [to another record] 
\end{verbatim}

Using these chunks, we can normalize for the fact that in the
dependencies produced by XTAG {\em borrowed} is dependent on {\em
shares} (i.e. in the same chunk) while in the Treebank {\em borrowed}
is directly dependent on the verb {\em rose}. That is to say, we are
looking at links between \underline{chunks}, not between
\underline{words}. The dependencies for the sentence are given below.

\begin{verbatim}
XTAG dependency    Treebank dependency

Borrowed::shares   Borrowed::rose 
shares::rose       shares::rose 
on::shares         on::shares 
the::Amex          the::Amex 
Amex::on           Amex::on 
rose::NIL          rose::NIL
to::rose           to::rose 
another::record    another::record 
record::to         record::to 
\end{verbatim}

After this normalization, testing simply consisted of counting how
many of the dependency links produced by XTAG matched the Treebank
dependency links. Due to some tokenization and subsequent alignment
problems we could only test on $835$ of the original $9891$ parsed
sentences. There were a total of $6135$ dependency links extracted
from the Treebank. The XTAG parses also produced $6135$ dependency
links for the same sentences. Of the dependencies produced by the XTAG
parser, $5165$ were correct giving us an accuracy of $84.2\%$.

%\section{Performance on various corpora}

%XTAG was used to parse Wall Street Journal (sentences of length $\leq$
%15 words), IBM manual, and ATIS corpora as a means of evaluating the
%coverage and correctness of XTAG parses. For this evaluation, a
%sentence is considered to have parsed if the correct parse is among
%the parses generated by XTAG. Verifying the presence of the correct
%parse among the generated parses is done manually at present by random
%sampling. The results are shown in Table~\ref{results}. 

%\begin{table}[ht]
%\begin{center}
%\begin{tabular}{|l|c|c|c|} \hline
%& \# of & & Av. \# of\\
%Corpus & Sentences & \% Parsed & Parses/Sent\\ \hline

%WSJ & 18,730 & 41.22 \% & 7.46 \\\hline

%IBM Manual & 2040 & 75.42\% & 6.12\\ \hline

%ATIS & 524 & 88.35\% & 6.0 \\ \hline 
%\end{tabular}
%\end{center}

%\caption{Performance of XTAG on various corpora}
%\label{results}
%\end{table}

%Performance on the WSJ corpus is lower relative to IBM and ATIS due
%to the wide-variety of syntactic constructions used. Even grammars
%induced on the partially bracketed WSJ corpus have fairly low
%performance (e.g. 57.1\% sentence accuracy for \cite{schabes93}).  

\section{Comparison with IBM}

The evaluation in this section was done with the earlier 1995 release
of the grammar. This section describes an experiment to measure the
crossing bracket accuracy of the XTAG-parsed IBM-manual sentences.  In
this experiment, XTAG parses of 1100 IBM-manual sentences have been
ranked using certain heuristics. The ranked parses have been
compared\footnote{We used the parseval program written by Phil Harison
  (phil@atc.boeing.com).}  against the bracketing given in the
Lancaster Treebank of IBM-manual sentences\footnote{The Treebank was
  obtained through Salim Roukos (roukos@watson.ibm.com) at IBM.}.
Table~\ref{ibm-results} shows the results of XTAG obtained in this
experiment, which used the highest ranked parse for each system. It
also shows the results of the latest IBM statistical grammar
(\cite{jelineketal94}) on the same genre of sentences. Only the
highest-ranked parse of both systems was used for this evaluation.
Crossing Brackets is the percentage of sentences with no pairs of
brackets crossing the Treebank bracketing (i.e.  (~(~a~b~)~c~) has a
crossing bracket measure of one if compared to (~a~(~b~c~)~)~). Recall
is the ratio of the number of constituents in the XTAG parse to the
number of constituents in the corresponding Treebank sentence.
Precision is the ratio of the number of correct constituents to the
total number of constituents in the XTAG parse.

\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|c|c|} \hline 
System & \# of & Crossing Bracket & Recall & Precision \\
& sentences & Accuracy & & \\ \hline
XTAG & 1100 & 81.29\% & 82.34\% & 55.37\% \\ \hline
IBM Statistical & 1100 & 86.20\% & 86.00\% & 85.00\% \\
grammar & &  &  &\\ \hline
\end{tabular}

\vspace{0.1in}

\caption{Performance of XTAG on IBM-manual sentences}
\label{ibm-results} 

\end{table}

As can be seen from Table~\ref{ibm-results}, the precision figure for
the XTAG system is considerably lower than that for IBM. For the
purposes of comparative evaluation against other systems, we had to
use the same crossing-brackets metric though we believe that the
crossing-brackets measure is inadequate for evaluating a grammar like
XTAG. There are two reasons for the inadequacy. First, the parse
generated by XTAG is much richer in its representation of the internal
structure of certain phrases than those present in manually created
treebanks (e.g. IBM: [$_N$ your personal computer], XTAG: [$_{NP}$
[$_G$ your] [$_N$ [$_N$ personal] [$_N$ computer]]]). This is
reflected in the number of constituents per sentence, shown in the
last column of Table~\ref{const-no}.\footnote{We are aware of the fact
  that increasing the number of constituents also increases the recall
  percentage. However we believe that this a legitimate gain.}

\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|c|c|} \hline
System & Sent. & \# of & Av. \# of & Av. \# of \\
& Length & sent & words/sent & Constituents/sent \\ \hline
XTAG & 1-10 & 654 & 7.45 & 22.03  \\ \cline{2-5}
& 1-15 & 978 & 9.13 & 30.56 \\ \hline
IBM Stat. & 1-10 & 447 & 7.50 & 4.60 \\ \cline{2-5}
Grammar & 1-15 & 883 & 10.30 & 6.40 \\ \hline
\end{tabular}
\caption{Constituents in XTAG parse and IBM parse}
\label{const-no}
\end{table}

A second reason for considering the crossing bracket measure
inadequate for evaluating XTAG is that the primary structure in XTAG
is the derivation tree from which the bracketed tree is derived. Two
identical bracketings for a sentence can have completely different
derivation trees (e.g. {\it kick the bucket} as an idiom vs. a
compositional use). A more direct measure of the performance of XTAG
would evaluate the derivation structure, which captures the
dependencies between words.

\section{Comparison with Alvey}

The evaluation in this section was done with the earlier 1995 release
of the grammar. This section compares XTAG to the Alvey Natural
Language Tools (ANLT) Grammar. We parsed the set of LDOCE Noun Phrases
presented in Appendix B of the technical report (\cite{Carroll93})
using XTAG.  Table~\ref{Alvey-xtag} summarizes the results of this
experiment.  A total of 143 noun phrases were parsed. The NPs which
did not have a correct parse in the top three derivations were
considered failures for either system. The maximum and average number
of derivations columns show the highest and the average number of
derivations produced for the NPs that have a correct derivation in the
top three.  We show the performance of XTAG both with and without the
tagger since the performance of the POS tagger is significantly
degraded on the NPs because the NPs are usually shorter than the
sentences on which it was trained. It would be interesting to see if
the two systems performed similarly on a wider range of data.

\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|c|c|c|}  \hline
% This table will compare the performance of the XTAG with Alvey.
System & \# of & \# parsed & \% parsed & Maximum & Average \\
& NPs &&& derivations & derivations \\ \hline
ANLT Parser & 143 & 127 & 88.81\% & 32 & 4.57 \\ \hline
XTAG Parser with & 143 & 93 & 65.03\% & 28 & 3.45 \\
POS tagger & & & & & \\ \hline
XTAG Parser without & 143 & 120 & 83.91\% & 28 & 4.14\\
POS tagger & & & & & \\ \hline
\end{tabular} \\

\vspace{0.1in}

\caption{Comparison of XTAG and ANLT Parser}
\label{Alvey-xtag}
\end{table}

%In \cite{Carroll93}, only the LDOCE NPs
%are annotated with the number of derivations; we are interested
%in getting more data annotated with this information, so that we can
%make further comparisons.

\section{Comparison with CLARE}

The evaluation in this section was done with the earlier 1995 release
of the grammar. This section compares the performance of XTAG against
that of the CLARE-2 system (\cite{clare-report92}) on the ATIS corpus.
Table~\ref{clare-results} shows the performance results. The
percentage parsed column for both systems represents the percentage of
sentences that produced any parse.  It must be noted that the
performance result shown for CLARE-2 is without any tuning of the
grammar for the ATIS domain. The performance of CLARE-3, a later
version of the CLARE system, is estimated to be 10\% higher than that
of the CLARE-2 system.\footnote{When CLARE-3 is tuned to the ATIS
  domain, performance increases to 90\%. However XTAG has not been
  tuned to the ATIS domain.}

\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|}  \hline
% This table will compare the performance of the XTAG with CLARE-2
System & Mean length & \% parsed \\ \hline
CLARE-2  & 6.53 & 68.50\% \\ \hline
XTAG  & 7.62 & 88.35\% \\ \hline
\end{tabular}
\caption{Performance of CLARE-2 and XTAG on the ATIS corpus}
\label{clare-results}
\end{table}

In an attempt to compare the performance of the two systems on a wider
range of sentences (from similar genres), we provide in
Table~\ref{clare-results1} the performance of CLARE-2 on LOB corpus
and the performance of XTAG on the WSJ corpus. The performance was
measured on sentences of up to 10 words for both systems.
\begin{table}[ht]
\centering
\begin{tabular}{|c|c|c|c|}  \hline
% This table will compare the performance of the XTAG with CLARE-2
System & Corpus & Mean length & \% parsed \\ \hline
CLARE-2 & LOB & 5.95 & 53.40\% \\ \hline
XTAG & WSJ & 6.00 & 55.58\% \\ \hline
\end{tabular}
\caption{Performance of CLARE-2 and XTAG on LOB and WSJ corpus
respectively}
\label{clare-results1}
\end{table}

